{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd22d10e-b5bc-4e09-8776-80f5d1803de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql.functions import date_format,to_date, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5ff0c8-52b4-4960-b919-6e99bf5b8a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x7fccbd3f9750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AWS_ENDPOINT = \"http://minio-webserver:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"U6dHDkvTv3CdrviA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"o8LTVjrVtNuzOv7DjGOxhN0HQDeksEej\"\n",
    "AWS_REGION =\"us-east-1\"\n",
    "\n",
    "BUCKET_STAGE=\"antaq-stage\"\n",
    "BUCKET_ANALYTICS=\"antaq-analytics\"\n",
    "\n",
    "SPARK_APP_NAME= \"dataviva-notebook-DW\"\n",
    "SPARK_MASTER_URL=\"spark://spark-master:7077\"\n",
    "\n",
    "clientS3 = boto3.client('s3',\n",
    "    endpoint_url=AWS_ENDPOINT,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "clientS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104fdd9f-2dff-48eb-8016-d99ab6ac5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterJars():\n",
    "    dir = \"/tmp/spark-jars\"\n",
    "    jars = os.listdir(dir)\n",
    "    stringJars = \"\"\n",
    "\n",
    "    for jar in jars:\n",
    "        print(f\"{dir}/{jar}\")\n",
    "        stringJars += f\"{dir}/{jar},\"\n",
    "    return stringJars[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf1b9e0-08ef-420e-8675-b19029bd15b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/spark-jars/slf4j-reload4j-2.0.6.jar\n",
      "/tmp/spark-jars/mysql-connector-j-8.1.0.jar\n",
      "/tmp/spark-jars/aws-java-sdk-dynamodb-1.12.400.jar\n",
      "/tmp/spark-jars/hadoop-aws-3.3.4.jar\n",
      "/tmp/spark-jars/httpclient5-5.2.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-kms-1.12.400.jar\n",
      "/tmp/spark-jars/aws-java-sdk-s3-1.12.400.jar\n",
      "/tmp/spark-jars/postgresql-42.6.0.jar\n",
      "/tmp/spark-jars/spark-hadoop-cloud_2.13-3.3.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-1.12.400.jar\n",
      "/tmp/spark-jars/mssql-jdbc-12.4.1.jre11.jar\n",
      "/tmp/spark-jars/aws-java-sdk-core-1.12.400.jar\n",
      "/tmp/spark-jars/spark-mssql-connector_2.12_3.0-1.0.0-alpha.jar\n",
      "/tmp/spark-jars/joda-time-2.12.2.jar\n",
      "/tmp/spark-jars/slf4j-api-2.0.6.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/02 23:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7e16752c312f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataviva-notebook-DW</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fccbd00e5d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client_spark.stop()\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(SPARK_APP_NAME)\n",
    "conf.setMaster(SPARK_MASTER_URL)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "        .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .set(\"spark.sql.sources.commitProtocolClass\",\n",
    "             \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .set(\"spark.jars\", obterJars()) \\\n",
    "        .set(\"spark.executor.memory\", \"3g\") \\\n",
    "        .set(\"spark.driver.memory\", \"3g\") \\\n",
    "        .set(\"spark.cores.max\", \"20\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "client_spark = SparkSession(sc)\n",
    "client_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c19c56e-6c09-48f7-bc6e-9273de1a8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDataFrameBancoDados(df, bancoDados:str, tabela: str, modoEscrita=\"overwrite\"):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://sql-server:1433;databaseName={bancoDados};\") \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", \"SA\") \\\n",
    "        .option(\"password\", \"SqlServer2019!\") \\\n",
    "        .option(\"encrypt\",False) \\\n",
    "        .mode(modoEscrita) \\\n",
    "        .save()\n",
    "    \n",
    "\n",
    "    logging.info(f\"{df.count()} registros salvos na tabela {tabela}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54407cb-4353-4fe9-bcdf-bc39faa6ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataframeFromSql(sparkSession: SparkSession, bancoDados: str, sql: str):\n",
    "    df = sparkSession.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\",  f\"jdbc:sqlserver://sql-server:1433;databaseName={bancoDados};\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"user\", \"SA\") \\\n",
    "        .option(\"password\", \"SqlServer2019!\") \\\n",
    "        .option(\"query\", sql) \\\n",
    "        .option(\"encrypt\",False) \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0a956-4c1a-4657-9ac2-baf245855941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarAnaliseTempoEspera():\n",
    "    SQL_ANALISE = \"\"\"\n",
    "        WITH atracacoes_mes as (\n",
    "        \t\tselect\n",
    "        \t\tdt.ano,\n",
    "        \t\tdt.mes,\n",
    "        \t\tdl.cod_uf ,\n",
    "        \t\tdl.regiao_hidrografica,\n",
    "        \t\tcount(*) qtd_atracacoes, \n",
    "        \t\tsum(af.tempo_espera_atracacao) tempo_espera_atracacao,\n",
    "        \t\tsum(af.tempo_atracado) tempo_atracado\n",
    "        \tfrom atracacoes_fato  af \n",
    "        \t\tleft join dim_tempo dt on CAST(af.dt_atracacao as date) = CAST (dt.[data] as date )\n",
    "        \t\tleft join dim_localizacao dl on dl.municipio =af.municipio \n",
    "        \t\tleft join dim_porto dp on dp.cod_porto_informante = af.cod_porto_informante \n",
    "        \twhere dt.ano in ('2023','2022','2021')\n",
    "        \t\tgroup by \n",
    "        \t\tdt.ano, dt.mes,\n",
    "        \t\tdl.cod_uf ,\n",
    "        \t\tdl.regiao_hidrografica\n",
    "        )\n",
    "        SELECT \n",
    "        \tcod_uf,\n",
    "        \tregiao_hidrografica,\n",
    "        \tano ano_atual, \n",
    "        \tmes mes_atual, \n",
    "        \tqtd_atracacoes, \n",
    "        \ttempo_espera_atracacao,\n",
    "        \ttempo_atracado,\n",
    "        \t LAG(ano,12) OVER ( ORDER BY ano, mes) AS ano_anterior, \n",
    "        \t LAG(mes,12) OVER ( ORDER BY ano, mes) AS mes_comparacao,\n",
    "        \t LAG(qtd_atracacoes,12) OVER ( ORDER BY ano, mes) AS qtd_atracacoes_periodo_anterior,\n",
    "        \t qtd_atracacoes - LAG(qtd_atracacoes,12) OVER (ORDER BY ano, mes) AS diferenca,\n",
    "        \t (qtd_atracacoes - LAG(qtd_atracacoes,12) OVER (ORDER BY ano, mes)) / CAST(qtd_atracacoes AS DECIMAL) AS variacao_percentual\n",
    "        from atracacoes_mes\n",
    "        where 1=1 \n",
    "        \t-- and cod_uf='CE'\n",
    "        order by ano,mes  \n",
    "    \"\"\"\n",
    "    dfAnalise = getDataframeFromSql(client_spark, \"antaq\", SQL_ANALISE)\n",
    "    logging.info(f\"Total registros de porto: {dfAnalise.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfAnalise.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Salvando análise no datalake\")\n",
    "    agora = datetime.now()\n",
    "    chave = f\"tempo_espera/year={agora.year}/month={agora.month}/day={agora.day}\"\n",
    "    dfArquivo.write.parquet(\n",
    "        path=f\"s3a://{BUCKET_ANALYTICS}/{chave}\",\n",
    "        mode=\"overwrite\")\n",
    "\n",
    "    logging.info(f\"Análise tempo de espera salvo no bucket analytics [chave].\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
