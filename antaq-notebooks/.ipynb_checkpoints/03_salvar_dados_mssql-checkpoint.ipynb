{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c664eb-5cd7-45c7-8495-4ace2243152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql.functions import date_format,to_date, to_timestamp, datediff,col\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b1f8a66-4bb6-4ac3-906c-77cad805d4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x7fd4578e1450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AWS_ENDPOINT = \"http://minio-webserver:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"U6dHDkvTv3CdrviA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"o8LTVjrVtNuzOv7DjGOxhN0HQDeksEej\"\n",
    "AWS_REGION =\"us-east-1\"\n",
    "\n",
    "BUCKET_STAGE=\"antaq-stage\"\n",
    "BUCKET_ANALYTICS=\"antaq-analytics\"\n",
    "\n",
    "SPARK_APP_NAME= \"dataviva-notebook-DW\"\n",
    "SPARK_MASTER_URL=\"spark://spark-master:7077\"\n",
    "\n",
    "clientS3 = boto3.client('s3',\n",
    "    endpoint_url=AWS_ENDPOINT,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "clientS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6970e719-0020-42b8-b4ea-907566da20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterJars():\n",
    "    dir = \"/tmp/spark-jars\"\n",
    "    jars = os.listdir(dir)\n",
    "    stringJars = \"\"\n",
    "\n",
    "    for jar in jars:\n",
    "        print(f\"{dir}/{jar}\")\n",
    "        stringJars += f\"{dir}/{jar},\"\n",
    "    return stringJars[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e1ee6e-145e-4784-9869-c25067f18adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/spark-jars/slf4j-reload4j-2.0.6.jar\n",
      "/tmp/spark-jars/mysql-connector-j-8.1.0.jar\n",
      "/tmp/spark-jars/aws-java-sdk-dynamodb-1.12.400.jar\n",
      "/tmp/spark-jars/hadoop-aws-3.3.4.jar\n",
      "/tmp/spark-jars/httpclient5-5.2.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-kms-1.12.400.jar\n",
      "/tmp/spark-jars/aws-java-sdk-s3-1.12.400.jar\n",
      "/tmp/spark-jars/postgresql-42.6.0.jar\n",
      "/tmp/spark-jars/spark-hadoop-cloud_2.13-3.3.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-1.12.400.jar\n",
      "/tmp/spark-jars/mssql-jdbc-12.4.1.jre11.jar\n",
      "/tmp/spark-jars/aws-java-sdk-core-1.12.400.jar\n",
      "/tmp/spark-jars/spark-mssql-connector_2.12_3.0-1.0.0-alpha.jar\n",
      "/tmp/spark-jars/joda-time-2.12.2.jar\n",
      "/tmp/spark-jars/slf4j-api-2.0.6.jar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7e16752c312f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataviva-notebook-DW</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd49d536b50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client_spark.stop()\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(SPARK_APP_NAME)\n",
    "conf.setMaster(SPARK_MASTER_URL)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "        .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .set(\"spark.sql.sources.commitProtocolClass\",\n",
    "             \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .set(\"spark.jars\", obterJars()) \\\n",
    "        .set(\"spark.executor.memory\", \"3g\") \\\n",
    "        .set(\"spark.driver.memory\", \"3g\") \\\n",
    "        .set(\"spark.cores.max\", \"20\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "client_spark = SparkSession(sc)\n",
    "client_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4b58e4-b58b-447f-b540-3e57050b5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDataFrameBancoDados(df, bancoDados, tabela: str, modoEscrita=\"overwrite\"):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://sql-server:1433;databaseName={bancoDados};\") \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", \"SA\") \\\n",
    "        .option(\"password\", \"SqlServer2019!\") \\\n",
    "        .option(\"encrypt\",False) \\\n",
    "        .mode(modoEscrita) \\\n",
    "        .save()\n",
    "    \n",
    "\n",
    "    logging.info(f\"{df.count()} registros salvos na tabela {tabela}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8676c55-c996-4e99-bd83-c936bdf1716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarArquivoAtracacao(dfArquivo, modoEscrita=\"overwrite\"):\n",
    "    dfArquivo.createOrReplaceTempView(\"tbl_atracacao\")\n",
    "    sqlQuery = \"\"\"\n",
    "        select \n",
    "            `IDAtracacao` id_atracacao,\n",
    "            `CDTUP` cod_porto_informante,\n",
    "            `IDBerco` id_berco,\n",
    "            `Berço` nome_berco,\n",
    "            `Porto Atracação` nome_porto_informante,\n",
    "            `Apelido Instalação Portuária` alias_porto_informante,\n",
    "            `Complexo Portuário` complexo_portuario,\n",
    "            `Tipo da Autoridade Portuária` tipo_autoridade_portuaria,\n",
    "            `Data Atracação` dt_atracacao,\n",
    "            `Data Chegada` dt_chegada,\n",
    "            `Data Desatracação` dt_desatracacao,\n",
    "            `Data Início Operação` dt_inicio_operacao,\n",
    "            `Data Término Operação` dt_fim_operacacao,\n",
    "            `Tipo de Operação` tipo_operacao,\n",
    "            `Tipo de Navegação da Atracação` tipo_navegacao,\n",
    "            `Nacionalidade do Armador` nacionalidade_armador,\n",
    "            `FlagMCOperacaoAtracacao` contabiliza_movimentacao,\n",
    "            `Terminal` terminal,\n",
    "            `Município` municipio,\n",
    "            `UF` nome_uf,\n",
    "            `SGUF` cod_uf,\n",
    "            `Região Geográfica` regiao_hidrografica,\n",
    "            `Nº da Capitania` cod_capitania,\n",
    "            `Nº do IMO` cod_IMO,\n",
    "            0 tempo_espera_atracacao,\n",
    "            0 tempo_espera_operacao,\n",
    "            0 tempo_operacao,\n",
    "            0 tempo_espera_desatracacao,\n",
    "            0 tempo_atracado,\n",
    "            0 tempo_estadia\n",
    "        from tbl_atracacao\n",
    "    \"\"\"\n",
    "\n",
    "    df = client_spark.sql(sqlQuery)\\\n",
    "        .withColumn(\"dt_atracacao\", to_timestamp(\"dt_atracacao\", \"dd/MM/yyyy HH:mm:ss\"))\\\n",
    "        .withColumn(\"dt_desatracacao\", to_timestamp(\"dt_desatracacao\", \"dd/MM/yyyy HH:mm:ss\"))\\\n",
    "        .withColumn(\"dt_chegada\", to_timestamp(\"dt_chegada\", \"dd/MM/yyyy hh:mm:ss\"))\\\n",
    "        .withColumn(\"dt_inicio_operacao\", to_timestamp(\"dt_inicio_operacao\", \"dd/MM/yyyy HH:mm:ss\"))\\\n",
    "        .withColumn(\"dt_fim_operacacao\", to_timestamp(\"dt_fim_operacacao\", \"dd/MM/yyyy HH:mm:ss\"))\\\n",
    "\n",
    "    df = df.withColumn(\"tempo_espera_atracacao\",(col(\"dt_atracacao\").cast(\"long\")-col(\"dt_chegada\").cast(\"long\"))/60)\\\n",
    "            .withColumn(\"tempo_espera_operacao\",(col(\"dt_inicio_operacao\").cast(\"long\")-col(\"dt_atracacao\").cast(\"long\"))/60)\\\n",
    "            .withColumn(\"tempo_operacao\",(col(\"dt_atracacao\").cast(\"long\")-col(\"dt_chegada\").cast(\"long\"))/60)\\\n",
    "            .withColumn(\"tempo_espera_desatracacao\",(col(\"dt_atracacao\").cast(\"long\")-col(\"dt_chegada\").cast(\"long\"))/60)\\\n",
    "            .withColumn(\"tempo_atracado\",(col(\"dt_atracacao\").cast(\"long\")-col(\"dt_chegada\").cast(\"long\"))/60)\\\n",
    "            .withColumn(\"tempo_estadia\",(col(\"dt_atracacao\").cast(\"long\")-col(\"dt_chegada\").cast(\"long\"))/60)\n",
    "\n",
    "    print(f\"Total registros no arquivo: {df.count()}\")\n",
    "\n",
    "    print(\"===========================================\")\n",
    "    # df.printSchema()\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    print(f\"Salvando dados de ATRACAÇÃO no banco de dados\")\n",
    "    salvarDataFrameBancoDados(df, \"antaq\", \"atracacao\",modoEscrita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502cf860-47e7-403e-8c32-098d4b22a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarArquivoCarga(dfArquivo, modoEscrita=\"overwrite\"):\n",
    "    dfArquivo.createOrReplaceTempView(\"tbl_carga\")\n",
    "    sqlQuery = \"\"\"\n",
    "        select \n",
    "            `IDCarga` id_carga,\n",
    "            `IDAtracacao` id_atracacao,\n",
    "            `Origem` cod_porto_origem,\n",
    "            `Destino` cod_porto_destino,\n",
    "            `CDMercadoria` cod_mercadoria,\n",
    "            `Tipo Operação da Carga` tipo_operacao,\n",
    "            `Carga Geral Acondicionamento` tipo_acondicionamento,\n",
    "            `ConteinerEstado` estado_container,\n",
    "            `Tipo Navegação` tipo_navegacao,\n",
    "            `FlagAutorizacao` flag_autorizacao,\n",
    "            `FlagCabotagem` flag_cabotagem,\n",
    "            `FlagCabotagemMovimentacao` flag_cabotagem_movimentacao,\n",
    "            `FlagConteinerTamanho` tamanho_container,\n",
    "            `FlagLongoCurso` flag_longo_curso,\n",
    "            `FlagMCOperacaoCarga` tipo_operacao_carga,\n",
    "            `FlagOffshore` flag_offshore,\n",
    "            `FlagTransporteViaInterioir` flag_via_interior,\n",
    "            `Percurso Transporte em vias Interiores` percurso_vias_interiores,\n",
    "            `Percurso Transporte Interiores` percurso_interiores,\n",
    "            `STNaturezaCarga` unica_natureza,\n",
    "            `STSH2` unico_capitulo,\n",
    "            `STSH4` unica_mercadoria,\n",
    "            `Natureza da Carga` natureza_carga,\n",
    "            `Sentido` sentido_operacao,\n",
    "            CAST(`TEU` AS INT) qtd_TEU_movimentacao,\n",
    "            CAST(`QTCarga` AS INT) qtd_unidades_movimentadas,\n",
    "            CAST(REPLACE(`VLPesoCargaBruta`,',','.') AS DOUBLE) peso_bruto,\n",
    "            CASE \n",
    "                WHEN `Carga Geral Acondicionamento` <> 'Conteinerizada'  THEN CAST(REPLACE(`VLPesoCargaBruta`,',','.') AS DOUBLE)\n",
    "                WHEN `ConteinerEstado` = 'Vazio' THEN 0\n",
    "                WHEN `FlagConteinerTamanho` = '20' THEN CAST(REPLACE(`VLPesoCargaBruta`,',','.') AS DOUBLE) - 2.12\n",
    "                WHEN `FlagConteinerTamanho` = '40' THEN CAST(REPLACE(`VLPesoCargaBruta`,',','.') AS DOUBLE) - 4\n",
    "                ELSE CAST(REPLACE(`VLPesoCargaBruta`,',','.') AS DOUBLE)\n",
    "            END peso_liquido\n",
    "        from tbl_carga\n",
    "    \"\"\"\n",
    "\n",
    "    df = client_spark.sql(sqlQuery)\n",
    "\n",
    "    print(f\"Total registros no arquivo: {df.count()}\")\n",
    "\n",
    "    print(\"===========================================\")\n",
    "    # df.printSchema()\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    print(f\"Salvando dados de CARGA no banco de dados\")\n",
    "    salvarDataFrameBancoDados(df, \"antaq\", \"carga\",modoEscrita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fd4c0d5-da78-4e4f-b767-caf464e87ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2023/atracacao/2023atracacao.csv\n",
      "Total registros no arquivo: 54463\n",
      "===========================================\n",
      "===========================================\n",
      "Salvando dados de ATRACAÇÃO no banco de dados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2023/carga/2023carga.csv\n"
     ]
    }
   ],
   "source": [
    "objetosS3 = clientS3.list_objects(Bucket=BUCKET_STAGE,Prefix=\"year=2023\")\n",
    "\n",
    "contArquivosAtracacao=0\n",
    "contArquivosCarga=0\n",
    "for obj in objetosS3.get(\"Contents\"):\n",
    "    chave = obj[\"Key\"]\n",
    "    ano, dataset, nomeArquivo = chave.split(\"/\")\n",
    "    \n",
    "    \n",
    "    print(chave)\n",
    "    dfArquivo = client_spark \\\n",
    "        .read \\\n",
    "        .csv(f\"s3a://{BUCKET_STAGE}/{chave}\", header=True,sep=\";\")\n",
    "\n",
    "    if dataset == \"atracacao\":\n",
    "        modoEscrita=\"overwrite\" if contArquivosAtracacao==0 else \"append\"\n",
    "        processarArquivoAtracacao(dfArquivo,modoEscrita)\n",
    "        contArquivosAtracacao+=1\n",
    "\n",
    "    # if dataset == \"carga\":\n",
    "    #     modoEscrita=\"overwrite\" if contArquivosCarga==0 else \"append\"\n",
    "    #     processarArquivoCarga(dfArquivo,modoEscrita)\n",
    "    #     contArquivosCarga+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "249e0834-b018-47d3-848a-79c3e0816ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
