{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd22d10e-b5bc-4e09-8776-80f5d1803de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql.functions import date_format,to_date, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5ff0c8-52b4-4960-b919-6e99bf5b8a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x7fccbd3f9750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AWS_ENDPOINT = \"http://minio-webserver:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"U6dHDkvTv3CdrviA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"o8LTVjrVtNuzOv7DjGOxhN0HQDeksEej\"\n",
    "AWS_REGION =\"us-east-1\"\n",
    "\n",
    "BUCKET_STAGE=\"antaq-stage\"\n",
    "BUCKET_ANALYTICS=\"antaq-analytics\"\n",
    "\n",
    "SPARK_APP_NAME= \"dataviva-notebook-DW\"\n",
    "SPARK_MASTER_URL=\"spark://spark-master:7077\"\n",
    "\n",
    "clientS3 = boto3.client('s3',\n",
    "    endpoint_url=AWS_ENDPOINT,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "clientS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104fdd9f-2dff-48eb-8016-d99ab6ac5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterJars():\n",
    "    dir = \"/tmp/spark-jars\"\n",
    "    jars = os.listdir(dir)\n",
    "    stringJars = \"\"\n",
    "\n",
    "    for jar in jars:\n",
    "        print(f\"{dir}/{jar}\")\n",
    "        stringJars += f\"{dir}/{jar},\"\n",
    "    return stringJars[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf1b9e0-08ef-420e-8675-b19029bd15b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/spark-jars/slf4j-reload4j-2.0.6.jar\n",
      "/tmp/spark-jars/mysql-connector-j-8.1.0.jar\n",
      "/tmp/spark-jars/aws-java-sdk-dynamodb-1.12.400.jar\n",
      "/tmp/spark-jars/hadoop-aws-3.3.4.jar\n",
      "/tmp/spark-jars/httpclient5-5.2.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-kms-1.12.400.jar\n",
      "/tmp/spark-jars/aws-java-sdk-s3-1.12.400.jar\n",
      "/tmp/spark-jars/postgresql-42.6.0.jar\n",
      "/tmp/spark-jars/spark-hadoop-cloud_2.13-3.3.1.jar\n",
      "/tmp/spark-jars/aws-java-sdk-1.12.400.jar\n",
      "/tmp/spark-jars/mssql-jdbc-12.4.1.jre11.jar\n",
      "/tmp/spark-jars/aws-java-sdk-core-1.12.400.jar\n",
      "/tmp/spark-jars/spark-mssql-connector_2.12_3.0-1.0.0-alpha.jar\n",
      "/tmp/spark-jars/joda-time-2.12.2.jar\n",
      "/tmp/spark-jars/slf4j-api-2.0.6.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/02 23:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7e16752c312f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataviva-notebook-DW</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fccbd00e5d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client_spark.stop()\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(SPARK_APP_NAME)\n",
    "conf.setMaster(SPARK_MASTER_URL)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "        .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .set(\"spark.sql.sources.commitProtocolClass\",\n",
    "             \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .set(\"spark.jars\", obterJars()) \\\n",
    "        .set(\"spark.executor.memory\", \"3g\") \\\n",
    "        .set(\"spark.driver.memory\", \"3g\") \\\n",
    "        .set(\"spark.cores.max\", \"20\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "client_spark = SparkSession(sc)\n",
    "client_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c19c56e-6c09-48f7-bc6e-9273de1a8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDataFrameBancoDados(df, bancoDados:str, tabela: str, modoEscrita=\"overwrite\"):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://sql-server:1433;databaseName={bancoDados};\") \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", \"SA\") \\\n",
    "        .option(\"password\", \"SqlServer2019!\") \\\n",
    "        .option(\"encrypt\",False) \\\n",
    "        .mode(modoEscrita) \\\n",
    "        .save()\n",
    "    \n",
    "\n",
    "    logging.info(f\"{df.count()} registros salvos na tabela {tabela}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54407cb-4353-4fe9-bcdf-bc39faa6ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataframeFromSql(sparkSession: SparkSession, bancoDados: str, sql: str):\n",
    "    df = sparkSession.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\",  f\"jdbc:sqlserver://sql-server:1433;databaseName={bancoDados};\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"user\", \"SA\") \\\n",
    "        .option(\"password\", \"SqlServer2019!\") \\\n",
    "        .option(\"query\", sql) \\\n",
    "        .option(\"encrypt\",False) \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5a155a-ed59-4c9f-98a4-b078c428adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDimensaoTempo():\n",
    "    SQL_RANGE_DATAS = \"\"\"\n",
    "    SELECT \n",
    "        min(dt_atracacao) data_inicial,\n",
    "        max(dt_atracacao) data_final\n",
    "    FROM atracacao\n",
    "    \"\"\"\n",
    "    dfRangeDatas = getDataframeFromSql(client_spark, \"antaq\", SQL_RANGE_DATAS)\n",
    "    dfDatas = dfRangeDatas.selectExpr(\"explode(sequence(data_inicial,data_final)) as data\")\n",
    "    dfDimTempo = dfDatas \\\n",
    "        .withColumn(\"ano\", date_format(dfDatas.data, \"yyyy\")) \\\n",
    "        .withColumn(\"mes\", date_format(dfDatas.data, \"MM\")) \\\n",
    "        .withColumn(\"dia\", date_format(dfDatas.data, \"dd\")) \\\n",
    "        .withColumn(\"ano_mes\", date_format(dfDatas.data, \"yyyy-MM\")) \\\n",
    "        .withColumn(\"trimestre\", date_format(dfDatas.data, \"yyyy-Q\"))\n",
    "    logging.info(f\"Total registros de tempo: {dfDimTempo.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfDimTempo.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela dimensão tempo no DW\")\n",
    "    salvarDataFrameBancoDados(dfDimTempo, \"antaq_dw\", \"dim_tempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9a6696-fccf-4b81-bfe8-091ffaff7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+---+-------+---------+\n",
      "|               data| ano|mes|dia|ano_mes|trimestre|\n",
      "+-------------------+----+---+---+-------+---------+\n",
      "|2000-10-11 14:00:00|2000| 10| 11|2000-10|   2000-4|\n",
      "|2000-10-12 14:00:00|2000| 10| 12|2000-10|   2000-4|\n",
      "|2000-10-13 14:00:00|2000| 10| 13|2000-10|   2000-4|\n",
      "|2000-10-14 14:00:00|2000| 10| 14|2000-10|   2000-4|\n",
      "|2000-10-15 14:00:00|2000| 10| 15|2000-10|   2000-4|\n",
      "+-------------------+----+---+---+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salvarDimensaoTempo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9208165-8abf-4726-b36e-cff227c3b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDimensaoPortoInformante():\n",
    "    SQL_DIM_PORTO = \"\"\"\n",
    "        SELECT\n",
    "        \tcod_porto_informante,cod_uf\n",
    "        FROM atracacao \n",
    "        where 1=1\n",
    "        group by cod_porto_informante, cod_uf   \n",
    "    \"\"\"\n",
    "    dfDimPorto = getDataframeFromSql(client_spark, \"antaq\", SQL_DIM_PORTO)\n",
    "    logging.info(f\"Total registros de porto: {dfDimPorto.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfDimPorto.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela dimensão porto no DW\")\n",
    "    salvarDataFrameBancoDados(dfDimPorto, \"antaq_dw\", \"dim_porto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae40869-cd82-4991-b13d-1d57604a2478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|cod_porto_informante|cod_uf|\n",
      "+--------------------+------+\n",
      "|             BRAM013|    AM|\n",
      "|             BRAM011|    AM|\n",
      "|             BRAM001|  null|\n",
      "|               BRMCZ|    AL|\n",
      "|               BRRIO|  null|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salvarDimensaoPortoInformante()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57683c11-045d-4fd6-91cc-af8fae840df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDimensaoLocalizacao():\n",
    "    SQL_DIM_LOCALIZACAO = \"\"\"\n",
    "        SELECT\n",
    "        \tmunicipio,cod_uf,nome_uf,regiao_hidrografica\n",
    "        FROM antaq.dbo.atracacao \n",
    "        where 1=1\n",
    "        group by nome_uf , cod_uf,municipio ,regiao_hidrografica  \n",
    "    \"\"\"\n",
    "    dfDimLocalizacao = getDataframeFromSql(client_spark, \"antaq\", SQL_DIM_LOCALIZACAO)\n",
    "    logging.info(f\"Total registros de proto: {dfDimLocalizacao.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfDimLocalizacao.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela dimensão localização no DW\")\n",
    "    salvarDataFrameBancoDados(dfDimLocalizacao, \"antaq_dw\", \"dim_localizacao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2557e1a-90ae-4240-98fc-a3b0cd4f3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "salvarDimensaoLocalizacao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0a211-9cd7-4bb2-8593-a115ef080a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarDimensaoBerco():\n",
    "    SQL_DIM_BERCO = \"\"\"\n",
    "        SELECT\n",
    "        \tid_berco,nome_berco\n",
    "        FROM antaq.dbo.atracacao \n",
    "        where 1=1\n",
    "        group by id_berco ,nome_berco  \n",
    "    \"\"\"\n",
    "    dfDimBerco = getDataframeFromSql(client_spark, \"antaq\", SQL_DIM_BERCO)\n",
    "    logging.info(f\"Total registros de berco: {dfDimBerco.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfDimBerco.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela dimensão porto no DW\")\n",
    "    salvarDataFrameBancoDados(dfDimBerco, \"antaq_dw\", \"dim_berco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139167f5-50ed-4ce2-9629-e103b3bc598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salvarDimensaoBerco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7999-1ffc-4e8f-a008-17de73e08c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarFatoAtracacao():\n",
    "    SQL_FATO_ATRACACOES = \"\"\"\n",
    "        SELECT \n",
    "            id_atracacao, \n",
    "            cod_porto_informante, \n",
    "            nome_porto_informante, \n",
    "            alias_porto_informante,\n",
    "            id_berco, \n",
    "            -- nome_berco, \n",
    "            complexo_portuario, \n",
    "            tipo_autoridade_portuaria, \n",
    "            dt_chegada, \n",
    "            dt_atracacao, \n",
    "            dt_inicio_operacao, \n",
    "            dt_fim_operacao, \n",
    "            dt_desatracacao, \n",
    "            CASE tipo_operacao\n",
    "                WHEN '1' THEN 'Movimentacão de Carga'\n",
    "                WHEN '2' THEN 'Passageiro'\n",
    "                WHEN '3' THEN 'Apoio'\n",
    "                WHEN '4' THEN 'Marinha'\n",
    "                WHEN '5' THEN 'Abastecimento'\n",
    "                WHEN '6' THEN 'Reparo/Manutenção'\n",
    "                WHEN '7' THEN 'Misto'\n",
    "                WHEN '8' THEN 'Retirada de Resíduos'\n",
    "                ELSE tipo_operacao\n",
    "            END tipo_operacao, \n",
    "            CASE tipo_navegacao \n",
    "                WHEN '1' THEN 'Navegação Interior'\n",
    "                WHEN '2' THEN 'Apoio Portuário'\n",
    "                WHEN '3' THEN 'Cabotagem'\n",
    "                WHEN '4' THEN 'Apoio Marítimo'\n",
    "                WHEN '5' THEN 'Longo Curso'\n",
    "                ELSE tipo_navegacao\n",
    "            END tipo_navegacao, \n",
    "            CASE nacionalidade_armador\n",
    "                WHEN '1' THEN 'Brasileira'\n",
    "                WHEN '2' THEN 'Estrangeira'\n",
    "                ELSE nacionalidade_armador\n",
    "            END nacionalidade_armador, \n",
    "            CASE contabiliza_movimentacao\n",
    "                WHEN '1' THEN 'S'\n",
    "                ELSE 'N'\n",
    "            END contabiliza_movimentacao, \n",
    "            terminal, \n",
    "            municipio, \n",
    "            -- nome_uf, \n",
    "            -- cod_uf, \n",
    "            -- regiao_hidrografica, \n",
    "            -- cod_capitania, \n",
    "            cod_IMO, \n",
    "            tempo_espera_atracacao, \n",
    "            tempo_espera_operacao, \n",
    "            tempo_operacao, \n",
    "            tempo_espera_desatracacao, \n",
    "            tempo_atracado, \n",
    "            tempo_estadia\n",
    "        FROM atracacao\n",
    "    \"\"\"\n",
    "    dfFatoAtracacao = getDataframeFromSql(client_spark, \"antaq\", SQL_FATO_ATRACACOES)\n",
    "    logging.info(f\"Total registros de atracacoes: {dfFatoAtracacao.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfFatoAtracacao.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela fato atracacoes no DW\")\n",
    "    salvarDataFrameBancoDados(dfFatoAtracacao, \"antaq_dw\", \"atracacoes_fato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2664c-0c68-4f07-8386-cc27c0d7c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "salvarFatoAtracacao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0c142-214a-4882-8682-53f24f7c9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarFatoCarga():\n",
    "    SQL_FATO_CARGA = \"\"\"\n",
    "        SELECT \n",
    "            id_carga, \n",
    "            c.id_atracacao, \n",
    "            cod_porto_origem, \n",
    "            cod_porto_destino, \n",
    "            cod_mercadoria, \n",
    "            c.tipo_operacao, \n",
    "            tipo_acondicionamento, \n",
    "            estado_container, \n",
    "            c.tipo_navegacao, \n",
    "            flag_autorizacao, \n",
    "            flag_cabotagem, \n",
    "            flag_cabotagem_movimentacao, \n",
    "            tamanho_container, \n",
    "            flag_longo_curso, \n",
    "            tipo_operacao_carga, \n",
    "            flag_offshore, \n",
    "            flag_via_interior, \n",
    "            percurso_vias_interiores, \n",
    "            percurso_interiores, \n",
    "            unica_natureza, \n",
    "            unico_capitulo, \n",
    "            unica_mercadoria, \n",
    "            natureza_carga, \n",
    "            sentido_operacao, \n",
    "            qtd_TEU_movimentacao, \n",
    "            qtd_unidades_movimentadas, \n",
    "            peso_bruto, \n",
    "            peso_liquido ,\n",
    "            dt_atracacao,\n",
    "            cod_porto_informante cod_porto_atracacao,\n",
    "            municipio,\n",
    "            YEAR(dt_atracacao) ano_atracacao, \n",
    "            MONTH(dt_atracacao) mes_atracacao,\n",
    "            cod_uf_atracacao\n",
    "        FROM carga c left join atracacao a on c.id_atracacao=a.id_atracacao\n",
    "    \"\"\"\n",
    "    dfFatoCarga = getDataframeFromSql(client_spark, \"antaq\", SQL_FATO_CARGA)\n",
    "    logging.info(f\"Total registros de atracacoes: {dfFatoCarga.count()}\")\n",
    "    print(\"===========================================\")\n",
    "    dfFatoCarga.show(5)\n",
    "    print(\"===========================================\")\n",
    "    logging.info(f\"Criando tabela fato atracacoes no DW\")\n",
    "    salvarDataFrameBancoDados(dfFatoCarga, \"antaq_dw\", \"cargas_fato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60053e87-8ac1-4602-a865-ec589431b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "salvarFatoCarga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55743762-598a-4c62-8030-188df8de65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
